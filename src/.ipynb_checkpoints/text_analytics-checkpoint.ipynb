{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "import openai\n",
    "\n",
    "from utils import get_completion, read_social_listening_data, join_text_batch, save_pandas_object, clean_text\n",
    "from credentials import OPENAI_API_KEY\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar parámetros\n",
    "topics_batch_size = 30\n",
    "sentiment_batch_size = 50\n",
    "num_topicos = 3\n",
    "column_name = \"text\"\n",
    "id_column = \"ID\"\n",
    "cliente = \"bancolombia_080623\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruta destino...../data/bancolombia_080623\n",
      "Número de filas inicial: 1200\n",
      "Número de filas después de eliminar duplicados: 1018\n"
     ]
    }
   ],
   "source": [
    "# Lectura de los datos\n",
    "df = read_social_listening_data(cliente)\n",
    "print(\"Número de filas inicial:\", len(df))\n",
    "df.drop_duplicates([column_name], inplace=True)\n",
    "print(\"Número de filas después de eliminar duplicados:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se crea una columna ID\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>public_metrics</th>\n",
       "      <th>context_annotations</th>\n",
       "      <th>entities</th>\n",
       "      <th>referenced_tweets</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-06-01T19:00:10.000Z</td>\n",
       "      <td>#MundialSub20 | ¡GOOOOL de Uruguay! A los 65' ...</td>\n",
       "      <td>{'retweet_count': 0, 'reply_count': 0, 'like_c...</td>\n",
       "      <td>[{'domain': {'id': '11', 'name': 'Sport', 'des...</td>\n",
       "      <td>{'annotations': [{'start': 1, 'end': 12, 'prob...</td>\n",
       "      <td>[{'type': 'replied_to', 'id': '166434107126079...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-06-01T19:00:18.000Z</td>\n",
       "      <td>#MundialSub20 \\n\\nGoooool de Uruguay\\n\\nAnders...</td>\n",
       "      <td>{'retweet_count': 0, 'reply_count': 0, 'like_c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'annotations': [{'start': 1, 'end': 12, 'prob...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-06-01T19:00:26.000Z</td>\n",
       "      <td>Que golazo para @Uruguay! Vamos!!\\n#MundialSub20</td>\n",
       "      <td>{'retweet_count': 0, 'reply_count': 0, 'like_c...</td>\n",
       "      <td>[{'domain': {'id': '11', 'name': 'Sport', 'des...</td>\n",
       "      <td>{'annotations': [{'start': 35, 'end': 46, 'pro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-06-01T19:00:45.000Z</td>\n",
       "      <td>Gol carajo !!!! Vamo los guachos !!! #uruguays...</td>\n",
       "      <td>{'retweet_count': 0, 'reply_count': 0, 'like_c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'annotations': [{'start': 38, 'end': 49, 'pro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-06-01T19:00:49.000Z</td>\n",
       "      <td>RT @marcadorec: Quién es Christian García Caja...</td>\n",
       "      <td>{'retweet_count': 1, 'reply_count': 0, 'like_c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'annotations': [{'start': 25, 'end': 46, 'pro...</td>\n",
       "      <td>[{'type': 'retweeted', 'id': '1664322399800811...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at  \\\n",
       "0  2023-06-01T19:00:10.000Z   \n",
       "1  2023-06-01T19:00:18.000Z   \n",
       "2  2023-06-01T19:00:26.000Z   \n",
       "3  2023-06-01T19:00:45.000Z   \n",
       "4  2023-06-01T19:00:49.000Z   \n",
       "\n",
       "                                                text  \\\n",
       "0  #MundialSub20 | ¡GOOOOL de Uruguay! A los 65' ...   \n",
       "1  #MundialSub20 \\n\\nGoooool de Uruguay\\n\\nAnders...   \n",
       "2   Que golazo para @Uruguay! Vamos!!\\n#MundialSub20   \n",
       "3  Gol carajo !!!! Vamo los guachos !!! #uruguays...   \n",
       "4  RT @marcadorec: Quién es Christian García Caja...   \n",
       "\n",
       "                                      public_metrics  \\\n",
       "0  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
       "1  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
       "2  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
       "3  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
       "4  {'retweet_count': 1, 'reply_count': 0, 'like_c...   \n",
       "\n",
       "                                 context_annotations  \\\n",
       "0  [{'domain': {'id': '11', 'name': 'Sport', 'des...   \n",
       "1                                                NaN   \n",
       "2  [{'domain': {'id': '11', 'name': 'Sport', 'des...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                            entities  \\\n",
       "0  {'annotations': [{'start': 1, 'end': 12, 'prob...   \n",
       "1  {'annotations': [{'start': 1, 'end': 12, 'prob...   \n",
       "2  {'annotations': [{'start': 35, 'end': 46, 'pro...   \n",
       "3  {'annotations': [{'start': 38, 'end': 49, 'pro...   \n",
       "4  {'annotations': [{'start': 25, 'end': 46, 'pro...   \n",
       "\n",
       "                                   referenced_tweets  ID  \n",
       "0  [{'type': 'replied_to', 'id': '166434107126079...   0  \n",
       "1                                                NaN   1  \n",
       "2                                                NaN   2  \n",
       "3                                                NaN   3  \n",
       "4  [{'type': 'retweeted', 'id': '1664322399800811...   4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definir ID\n",
    "if id_column in df.columns:\n",
    "    print(f\"La columna ID {(id_column)} ya existe\")\n",
    "else:\n",
    "    print(\"Se crea una columna ID\")\n",
    "    df[id_column] = range(0, len(df))\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar el texto\n",
    "df[\"text\"] = clean_text(df, \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de Tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(0, len(df), topics_batch_size):\n",
    "    text_batch = df[i : (i + topics_batch_size)][column_name].tolist()\n",
    "    str_text_batch = join_text_batch(text_batch)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Determine máximo {num_topicos} tópicos para \\\n",
    "        cada una de las frases a continuación: \\\n",
    "\n",
    "        {str_text_batch}\n",
    "\n",
    "        Cada tópico debe ser de máximo tres palabras.\n",
    "        El resultado debe ser un JSON con cada frase y su lista de tópicos.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    response_i = get_completion(prompt)\n",
    "    response.append(response_i)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"Iteración:\", i) \n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"topic_list.pickle\", \"wb\") as file:\n",
    "    pickle.dump(response, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"topic_list.pickle\", \"rb\") as file:\n",
    "    loaded_list = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiar las llaves de los diccionarios por el ID correspondiente\n",
    "new_dict = {}\n",
    "\n",
    "for i in range(0, len(response)):\n",
    "    if i != 25: # verificar, esto no puede suceder en todos los casos\n",
    "        dict_i = json.loads(''.join(response[i].splitlines()))\n",
    "        ini = i * topics_batch_size\n",
    "        fin = topics_batch_size * (i + 1)\n",
    "        new_keys = df[ini : fin].ID.tolist()\n",
    "\n",
    "        new_dict_i = {}\n",
    "        for i, (key, value) in enumerate(dict_i.items(), start=0):\n",
    "            new_key = new_keys[i]\n",
    "            new_dict_i[new_key] = value\n",
    "\n",
    "        new_dict.update(new_dict_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular número de tópicos por ID\n",
    "element_lengths = []\n",
    "\n",
    "for value in new_dict.values():\n",
    "    element_lengths.append(len(value))\n",
    "\n",
    "print(\"Print max: \", max(element_lengths))\n",
    "print(\"Print min: \", min(element_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cual = list(map(lambda x: x < 3, element_lengths))\n",
    "shorter_keys = pd.Series(new_dict.keys())[cual]\n",
    "print(\"Número de llaves con menos de 3 tópicos:\", len(shorter_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volver todas las listas de tamaño 3\n",
    "for key in new_dict.keys():\n",
    "    \n",
    "    if key in shorter_keys:\n",
    "        values = new_dict[key]\n",
    "        len_ = len(values)\n",
    "        new_dict[key] = new_dict[key] + [\"\"] * (max(element_lengths) - len_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular número de tópicos por ID\n",
    "element_lengths = []\n",
    "\n",
    "for value in new_dict.values():\n",
    "    element_lengths.append(len(value))\n",
    "\n",
    "print(\"Print max: \", max(element_lengths))\n",
    "print(\"Print min: \", min(element_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casos particulares\n",
    "new_dict[801] = new_dict[801] + [\"\"]\n",
    "new_dict[802] = new_dict[802] + [\"\"]\n",
    "new_dict[804] = new_dict[804] + [\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular número de tópicos por ID\n",
    "element_lengths = []\n",
    "\n",
    "for value in new_dict.values():\n",
    "    element_lengths.append(len(value))\n",
    "\n",
    "print(\"Print max: \", max(element_lengths))\n",
    "print(\"Print min: \", min(element_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir fecha de creación del tweet\n",
    "topics_df = pd.DataFrame(new_dict)\n",
    "topics_df = topics_df.transpose()\n",
    "topics_df.reset_index(inplace=True)\n",
    "topics_df.columns = [\"ID\", \"Topicp_1\", \"Topico_2\", \"Topico_3\"]\n",
    "topics_df = topics_df.merge(df[[\"ID\", \"created_at\"]])\n",
    "topics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir métricas públicas\n",
    "public_metrics = df.public_metrics.map(lambda s: ast.literal_eval(s)) \n",
    "public_metrics_df = pd.DataFrame(public_metrics.tolist())\n",
    "public_metrics_df[\"ID\"] = df[\"ID\"]\n",
    "topics_df = topics_df.merge(public_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar DataFrame procesado \n",
    "save_pandas_object(topics_df, root_path=\"../artifacts\", subfolder=\"bancolombia_080623\", name=\"tweet_social_listening_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de sentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(0, len(df), sentiment_batch_size):\n",
    "    text_batch = df[i : (i + sentiment_batch_size)].text.tolist()\n",
    "    str_text_batch = join_text_batch(text_batch)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Determine el sentimiento para \\\n",
    "        cada una de las frases a continuación: \\\n",
    "\n",
    "        {str_text_batch}\n",
    "\n",
    "        El resultado debe ser un JSON con cada frase y su sentimiento.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    response_i = get_completion(prompt)\n",
    "    response.append(response_i)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"Iteración:\", i)\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sentiment_list.pickle\", \"wb\") as file:\n",
    "    pickle.dump(response, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sentiment_list.pickle\", \"rb\") as file:\n",
    "    loaded_list = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
